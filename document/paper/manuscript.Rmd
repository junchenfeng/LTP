---
title: "Mixture Learning Curve with Survival Selection"
author: "Junchen Feng"
date: "Thursday, April 28, 2016"
header-includes:
   - \usepackage{setspace}
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \doublespacing
output: pdf_document
---

# I.Introduction
Learning curve is one of key concepts in learning analytics. Explicitly, it describes the relationship between error rate and number of practice. Implicitly, it models the dynamics of mastering a latent skill in the class of task where practice makes perfect (PMP task). The basic training for many disciplines are PMP task, including but not limited to elementary math, beginner lanaguage learning and basic computer science. In principle, PMP task can extend to all repetive tasks that aim for muscle memory.

The key decision for PMP learning task is the stop condition. One can either stop when no progress can be expected or when the expected success rate is achieved. The former stop condition requires the knowledge of the slope of the learning curve while the later requires that of the level of the learning curve. Therefore, it is crucial to identify the learning curve of the PMP task.

# II.Literature Review
The seminal study of learning curve conducted by Colbert and Anderson **Citation** gave birth to the popular Bayesian Knowedge Tracing model (BKT). BKT assumes that the learning curve is an exponetial function, allowing for guess and slippage as measure error.

The BKT model inspired a family of latent models that is a variation of the Hidden Markov Model(HMM) where the dynamics of the latent mastery is described by a parametric model. Notably.

Matthew Streeter(2015) proposes a non-parametric model that does not make any strong parametric assumption, which he termed as mixture learning curve model (MLC). Matthew has shown that the BKT family is a special case of MLC and thus the performance of MLC is weakly stronger than the BKT family, when the number of componenent is correctly specified. He also reports that the MLC applied in Duolingo has out performed the BKT family. Although Matthew Streeter provides the pesudo-code for the discrete version of the MLC, he leaves out key step to pin down a unique set of mixture parameters. As the model current stands, it is not uniquely identified and thus suffers from stability problem.

In addition, both BKT family and MLC model assumes that user composition is homogeneous for all support of practice numbers. If different type of learners has different hazard rate, both BKT and MLC estimates will be biased. No solution is current avaiable at hand.


# III.Parameter Identifcation 

## Bayesian Knowledge Tracing Model
Following Van De Sande(2013)'s formulation, cannoical bayesian knowledge tracing(bkt) model can be written as 

$$
P(C_t) = c - Ae^{-\beta t}
$$

$P(C_t)$ is the probability of a correct answer at practice period t. $C=1-P(S)$ where $P(S)$ is the slippage probability. $A=(1-P(S)-P(G))*(1-P(L_0)$  where $P(G)$ is the guess probability, $P(L_0)$ is the initial latent mastery. $\beta = -log(1-P(T))$ where $P(T)$ is the learn rate. 

Such specification cannot identify $P(G)$ and $P(L_0)$. In addition, Because $\beta$ dominates the gradient vector, $P(S)$ and ($P(G)$, $P(L_0)$) is stably identified.



## Mixture Learning Curve Model

The key feature of MLC model is that it foregos explicit measures of latent knowledge mastery. Instead, it uses error rate as a proxy of mastery and describes the learner status with an error rate profile. 

MLC models assumes there are N different learning curve, each describing a relationship of error rate and practice opportunties. Users is described as a mixture of such learning curve, whose weights sum to 1. 


The log likelihood function for the observed data is 
$$
\mathcal{L} = \sum_{s}\{log[\sum_{j}p_j\prod_{t}\mathcal{B}(q_t^j, v_t)]\}
$$

$p_j$ is user profile's mixture weight. $q_t^j$ is probability of correct answer at practice opportunity $t$ for $j^{th}$ learning curve, and $v_t$ is the response at practice opportunity $t$. $\mathcal{B}(*,*)$ is the Bernoulli distribution. 

To increase stability of the estimation, Streeter(2015) suggests to shrink the estimated $\mathcal{B}(*,*)$ toward a beta prior. The $p_j$ is initialized as equal weight and $q_t^j$ as uniform random variable between (0,1).  

The current model specification, with $nt$ actual data points, there are $k(n+1)t$ parameters to be estimated. without extra constraints on the learning curve and mixture weight, the MLC is not uniquely identified.   The current routine estimates five separate models and chooses the parameter set that maximizes l2 norm of learning curves so that the model has the strongest power to differentiate learner type. 



# V.Performance Comparison

For practical purpose, parameter identification is less useful than outsample prediction. Therefore, the performance metrics of interest is Area Under Curve(AUC). The performance is compared with both simulated data whose dgp is known and field data whose dgp is unknown.

## 5.1 Simulation

```{r,message=FALSE, warning=FALSE,echo=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)
```

## 5.1.1 Data Generating Process
The respective model is trained on 90% of the data, while the auc metric is evaluated on 10% of the holdout data. 1000 users are simulated. Each user has a maximum of 10 data points.

The first dataset follows the bayesian knowledge tracing process. Slip is 10%, guess is 20%, the initial mastery is 60% and the learn rate is 20%.

The second dataset follows a learning curve of (0.85,0.7,0.6,0.5,0.4,0.3,0.25,0.20,0.15,0.1), which implies same slip rate and guess rate as the single component BKT.

The third dataset follows a mixture of learning curves. One learning curve is the same as single component MLC, which represents a progressing learner. The other curve is a stable 10% error rate which represents an accomplished learner. 80% of the simulated user is progressing user while the rest 20% is accomplished users.

The average learning curves of three datasets are show in the following graphs.

```{r, message=FALSE,warning=FALSE,echo=FALSE}
lc_1 = data.frame(t=seq(1,10), r=as.numeric(10),type='d1')
lc_1$r[1] = 0.6
for (i in seq(2,10)){
    lc_1$r[i] = lc_1$r[i-1] + (1-lc_1$r[i-1])*0.2
}
lc_1$r = 1-lc_1$r

lc_2 = data.frame(t=seq(1,10), r=c(0.85,0.7,0.6,0.5,0.4,0.3,0.25,0.20,0.15,0.1), type='d2')
lc_3 = data.frame(t=seq(1,10), r=c(0.85,0.7,0.6,0.5,0.4,0.3,0.25,0.20,0.15,0.1)*0.7+c(0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1)*0.3, type='d3')

lc = rbind(lc_1,lc_2,lc_3)

qplot(data=lc, x=t,y=r,col=type, geom='line')

```

## 5.1.2 Performance on the Complete Dataset

Mixture Learning Curve dominates all three datasets in terms of auc. 

```{r,message=FALSE, warning=FALSE,echo=FALSE}
file_path = 'C:/Users/junchen/Documents/GitHub/pyMLC/data/test/pk.txt'
pk_data = read.table(file_path, sep=',', col.names=c('fpr','tpr','method','exp'))

qplot(data=pk_data, x=fpr, y=tpr, geom='line', col=factor(method), facets=.~exp)

```

## 5.2 Field Data

## 5.2.1 English Data

## 5.2.2 Math Data

# VI. Future Research
Learning curve can be used to generate learning task recommendation that optimize the learning proficiency.



# Reference